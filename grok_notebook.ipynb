{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:06.460206Z",
     "start_time": "2025-02-26T16:12:06.447575Z"
    }
   },
   "source": [
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "from torch import device\n",
    "\n",
    "from plot import get_parameter_norms, plot_dicts\n",
    "\n",
    "%matplotlib inline\n",
    "# %debug"
   ],
   "outputs": [],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:08.204025Z",
     "start_time": "2025-02-26T16:12:08.199055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "from itertools import permutations\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import xscale\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import device\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ],
   "id": "cf236d5c5254062",
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c76bc95bb7534eab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:10.684072Z",
     "start_time": "2025-02-26T16:12:10.673204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Causal transformer block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_mask = torch.full(\n",
    "            (len(x), len(x)), -float(\"Inf\"), device=x.device, dtype=x.dtype\n",
    "        )\n",
    "        attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "        attn_mask[torch.isnan(attn_mask)] = 0.0 # fixes all 'nan' on 'mps' device\n",
    "\n",
    "        x = self.ln_1(x)\n",
    "        a, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
    "        x = x + a\n",
    "        m = self.mlp(self.ln_2(x))\n",
    "        x = x + m\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Causal Transformer decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=128, num_layers=2, num_heads=4, num_tokens=97, seq_len=5):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_tokens, dim)\n",
    "        self.position_embeddings = nn.Embedding(seq_len, dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(Block(dim=dim, num_heads=num_heads))\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, num_tokens, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.token_embeddings(x)\n",
    "        positions = torch.arange(x.shape[0], device=x.device).unsqueeze(-1)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "\n",
    "        h = self.ln_f(h)\n",
    "        logits = self.head(h)\n",
    "        return logits\n",
    "\n"
   ],
   "id": "b5c530c2a5b82cde",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:12.336152Z",
     "start_time": "2025-02-26T16:12:12.333663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_plot_infix(args):\n",
    "    # plot model architecture infix\n",
    "    ff = datetime.now().strftime(\"%f\")\n",
    "    plot_infix = f\"l{args.num_layers}_h{args.num_heads}_e{args.embedding}_{ff}\"\n",
    "    return plot_infix"
   ],
   "id": "b0840ce64fb06732",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:16.697215Z",
     "start_time": "2025-02-26T16:12:16.685330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# replace with read_args(sys.argv[1:]) in python\n",
    "def read_args(args):\n",
    "    parser = ArgumentParser(description=\"Grokfast\")\n",
    "    \n",
    "    print(f\"provided args: {args}\")\n",
    "    \n",
    "    # architecture parameters\n",
    "    parser.add_argument(\"--embedding\", type=int, default=128)\n",
    "    parser.add_argument(\"--num_layers\", type=int, default=2)\n",
    "    parser.add_argument(\"--num_heads\", type=int, default=4)\n",
    "\n",
    "    # run params\n",
    "    parser.add_argument(\"--label\", default=\"\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    parser.add_argument(\"--p\", type=int, default=97)\n",
    "    parser.add_argument(\"--budget\", type=int, default=3e5)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=512)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--beta1\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--beta2\", type=float, default=0.98)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0)\n",
    "    parser.add_argument(\"--optimizer\", default=\"Adam\")\n",
    "\n",
    "    # Grokfast\n",
    "    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"none\")\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--window_size\", type=int, default=100)\n",
    "    parser.add_argument(\"--lamb\", type=float, default=5.0)\n",
    "\n",
    "    # Ablation studies\n",
    "    parser.add_argument(\"--two_stage\", action='store_true')\n",
    "    parser.add_argument(\"--save_weights\", action='store_true')\n",
    "\n",
    "    args = parser.parse_args(args=args)\n",
    "    \n",
    "    args.plot_infix = get_plot_infix(args=args)\n",
    "    \n",
    "    filter_str = ('_' if args.label != '' else '') + args.filter\n",
    "    window_size_str = f'_w{args.window_size}'\n",
    "    alpha_str = f'_a{args.alpha:.3f}'.replace('.', '')\n",
    "    lamb_str = f'_l{int(args.lamb)}'\n",
    "\n",
    "    if args.filter == 'none':\n",
    "        filter_suffix = ''\n",
    "    elif args.filter == 'ma':\n",
    "        filter_suffix = window_size_str + lamb_str\n",
    "    elif args.filter == 'ema':\n",
    "        filter_suffix = alpha_str + lamb_str\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized filter type {args.filter}\")\n",
    "\n",
    "    optim_suffix = ''\n",
    "    if args.weight_decay != 0:\n",
    "        optim_suffix = optim_suffix + f'_wd{args.weight_decay:.1e}'.replace('.', '')\n",
    "    if args.lr != 1e-3:\n",
    "        optim_suffix = optim_suffix + f'_lrx{int(args.lr / 1e-3)}'\n",
    "\n",
    "    args.label = args.label + filter_str + filter_suffix + optim_suffix\n",
    "    print(f'Experiment results saved under name: {args.label}')\n",
    "\n",
    "    \n",
    "    return args"
   ],
   "id": "3f2627680c3e6c3a",
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:18.455462Z",
     "start_time": "2025-02-26T16:12:18.450316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "simulated_args = ['--embedding', '64', '--num_heads', '2']\n",
    "print(simulated_args)\n",
    "args = read_args(simulated_args)"
   ],
   "id": "3f37b30b79252c07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--embedding', '64', '--num_heads', '2']\n",
      "provided args: ['--embedding', '64', '--num_heads', '2']\n",
      "Experiment results saved under name: none\n"
     ]
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:35:53.933237Z",
     "start_time": "2025-02-25T10:35:53.922482Z"
    }
   },
   "cell_type": "code",
   "source": "args",
   "id": "b4b7ea46d3335e73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(embedding=64, num_layers=2, num_heads=2, label='none', seed=0, p=97, budget=300000.0, batch_size=512, lr=0.001, beta1=0.9, beta2=0.98, weight_decay=0, optimizer='Adam', filter='none', alpha=0.99, window_size=100, lamb=5.0, two_stage=False, save_weights=False, plot_infix='l2_h2_e64_696935')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:38:39.691826Z",
     "start_time": "2025-02-25T10:38:39.684291Z"
    }
   },
   "cell_type": "code",
   "source": "args.p",
   "id": "31cfa6b10723f1c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:22.963885Z",
     "start_time": "2025-02-26T16:12:22.959253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def multiplication_mod_p_data(p, eq_token, op_token):\n",
    "    \"\"\"x◦y = x/y (mod p) for 0 ≤ x < p, 0 < y < p\n",
    "    \"\"\"\n",
    "    x = torch.arange(p)\n",
    "    y = torch.arange(1, p)\n",
    "    x, y = torch.cartesian_prod(x, y).T\n",
    "\n",
    "    eq = torch.ones_like(x) * eq_token\n",
    "    op = torch.ones_like(x) * op_token\n",
    "    result = x * y % p\n",
    "\n",
    "    # \"All of our experiments used a small transformer trained on datasets of\n",
    "    # equations of the form a◦b = c, where each of “a”, “◦”, “b”, “=”, and “c”\n",
    "    # is a seperate token\"\n",
    "    return torch.stack([x, op, y, eq, result])"
   ],
   "id": "327ca3e0d1cf36e4",
   "outputs": [],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T15:47:38.579265Z",
     "start_time": "2025-02-26T15:47:38.568898Z"
    }
   },
   "cell_type": "code",
   "source": "data = multiplication_mod_p_data(args.p, args.p, args.p)",
   "id": "727de19cb548ef2a",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:49:08.041178Z",
     "start_time": "2025-02-25T10:49:08.019799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = 5891\n",
    "data[:, n:n+10]"
   ],
   "id": "cb4e822e0784f437",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[61, 61, 61, 61, 61, 61, 61, 61, 61, 61],\n",
       "        [97, 97, 97, 97, 97, 97, 97, 97, 97, 97],\n",
       "        [36, 37, 38, 39, 40, 41, 42, 43, 44, 45],\n",
       "        [97, 97, 97, 97, 97, 97, 97, 97, 97, 97],\n",
       "        [62, 26, 87, 51, 15, 76, 40,  4, 65, 29]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:37.406262Z",
     "start_time": "2025-02-26T16:12:37.401410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_model(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # tokens for <op> and <=>. It's not clear why <=> is needed at all since it\n",
    "    # has no effect on the output, but we'll leave it in to best follow the\n",
    "    # paper.\n",
    "    eq_token = args.p\n",
    "    op_token = args.p + 1\n",
    "\n",
    "    # \"We trained a standard decoder-only transformer (Vaswani et al., 2017)\n",
    "    # with causal attention masking, and calculated loss and accuracy only on\n",
    "    # the answer part of the equation. For all experiments we used a\n",
    "    # transformer with 2 layers, width 128, and 4 attention heads\"\n",
    "    model = Decoder(\n",
    "        dim=args.embedding,\n",
    "        num_layers=args.num_layers,\n",
    "        num_heads=args.num_heads,\n",
    "        num_tokens=args.p + 2,\n",
    "        seq_len=5\n",
    "    ).to(device)\n",
    "    print_model(model)\n",
    "    return model"
   ],
   "id": "771557c44fa31af8",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:40.935992Z",
     "start_time": "2025-02-26T16:12:40.932846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_model(model):\n",
    "    nparams = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "    print(model)\n",
    "    print(f'Total number of parameters: {nparams}')\n",
    "    return"
   ],
   "id": "127c8b1cfff068e1",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:44.994882Z",
     "start_time": "2025-02-26T16:12:44.989900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, token_array, labels=None):\n",
    "        \"\"\"\n",
    "        token_array: numpy array where each column is a sequence of token IDs for one example\n",
    "        labels: optional array of labels for each example\n",
    "        \"\"\"\n",
    "        self.data = torch.tensor(token_array.T)  # Transpose to make each row an example\n",
    "        self.labels = None if labels is None else torch.tensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.data[idx], self.labels[idx]\n",
    "        return self.data[idx]"
   ],
   "id": "2e01ac957170d5e4",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:47.832456Z",
     "start_time": "2025-02-26T16:12:47.827477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_dataloader(token_array, labels=None):\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TransformerDataset(token_array, labels)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    return dataloader"
   ],
   "id": "2ba435bdca2d34e",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:12:59.410847Z",
     "start_time": "2025-02-26T16:12:59.400380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eq_token = args.p\n",
    "op_token = args.p + 1\n",
    "data = multiplication_mod_p_data(p=args.p, eq_token=args.p, op_token=args.p)\n"
   ],
   "id": "f256bc1fb34dcafc",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T15:48:22.904231Z",
     "start_time": "2025-02-26T15:48:22.892903Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = TransformerDataset(data.clone().detach().numpy())",
   "id": "ded832628f82df3b",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T13:00:59.441815Z",
     "start_time": "2025-02-25T13:00:59.438306Z"
    }
   },
   "cell_type": "code",
   "source": "dataloader = build_dataloader(token_array=data.clone().detach().numpy())",
   "id": "ea3f33479dc198d9",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:13:06.316707Z",
     "start_time": "2025-02-26T16:13:06.310131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, token_array, labels=None):\n",
    "        self.data = torch.tensor(token_array.T)  # Transpose to make each row an example\n",
    "        self.labels = None if labels is None else torch.tensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.data[idx], self.labels[idx]\n",
    "        return self.data[idx]\n",
    "\n",
    "def create_train_test_dataloaders(token_array, labels=None, train_ratio=0.8, batch_size=32):\n",
    "    dataset = TransformerDataset(token_array, labels)\n",
    "    \n",
    "    # Calculate lengths for split\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, test_size], \n",
    "        generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ],
   "id": "3ad78205b150249",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:13:09.579414Z",
     "start_time": "2025-02-26T16:13:09.567973Z"
    }
   },
   "cell_type": "code",
   "source": "train_loader, test_loader = create_train_test_dataloaders(token_array=data.clone().detach().numpy(), train_ratio=0.5)",
   "id": "6b902a62c1e2e28c",
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T16:34:20.519400Z",
     "start_time": "2025-02-25T16:34:20.513069Z"
    }
   },
   "cell_type": "code",
   "source": "len(dataset)",
   "id": "ff105a7c12118133",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9312"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T13:13:48.558885Z",
     "start_time": "2025-02-25T13:13:48.536535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Usage in training loop\n",
    "for batch in train_loader:\n",
    "    if len(batch) == 2:\n",
    "        inputs, targets = batch\n",
    "    else:\n",
    "        inputs = batch\n",
    "    pass\n",
    "    # Forward pass, loss calculation, etc."
   ],
   "id": "d56f409b6fa22a1e",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:13:16.855582Z",
     "start_time": "2025-02-26T16:13:16.848191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLogger:\n",
    "    def __init__(self):\n",
    "        # Initialize an empty dictionary to store logs\n",
    "        self.logs = {}\n",
    "\n",
    "    def log_data(self, category, key, value):\n",
    "        \"\"\"\n",
    "        Log data under a specific category and key.\n",
    "\n",
    "        :param category: The category or subset of data logs.\n",
    "        :param key: The key within the category to store the value.\n",
    "        :param value: The value to log.\n",
    "        \"\"\"\n",
    "        if category not in self.logs:\n",
    "            self.logs[category] = {}\n",
    "        self.logs[category][key] = value\n",
    "\n",
    "    def update_category(self, category, data_dict):\n",
    "        \"\"\"\n",
    "        Add or update a whole sub-dictionary for a specific category.\n",
    "\n",
    "        :param category: The category to update.\n",
    "        :param data_dict: The dictionary containing data to add or update.\n",
    "        \"\"\"\n",
    "        if category not in self.logs:\n",
    "            self.logs[category] = {}\n",
    "        self.logs[category].update(data_dict)\n",
    "        \n",
    "    def update_category_means(self, category, data_dict):\n",
    "        # info compute means of category dict and update  category dict\n",
    "        for key in data_dict:\n",
    "            data_dict[key] = data_dict[key].mean()\n",
    "        self.update_category(category, data_dict)\n",
    "        \n",
    "    def get_all_logs(self):\n",
    "        return self.logs.copy()\n",
    "\n",
    "    def get_logs(self, category=None):\n",
    "        \"\"\"\n",
    "        Retrieve logs for a specific category or all logs if no category is specified.\n",
    "\n",
    "        :param category: The category to retrieve logs for. If None, retrieve all logs.\n",
    "        :return: The logs for the specified category or all logs.\n",
    "        \"\"\"\n",
    "        if category:\n",
    "            return self.logs.get(category, {})\n",
    "        return self.logs\n",
    "\n",
    "    def clear_logs(self, category=None):\n",
    "        \"\"\"\n",
    "        Clear logs for a specific category or all logs if no category is specified.\n",
    "\n",
    "        :param category: The category to clear logs for. If None, clear all logs.\n",
    "        \"\"\"\n",
    "        if category:\n",
    "            if category in self.logs:\n",
    "                del self.logs[category]\n",
    "        else:\n",
    "            self.logs.clear()\n",
    "\n",
    "# # Example usage\n",
    "# logger = DataLogger()\n",
    "# logger.log_data('temperature', 'sensor1', 25.5)\n",
    "# logger.log_data('temperature', 'sensor2', 26.0)\n",
    "# \n",
    "# # Update the 'temperature' category with a new sub-dictionary\n",
    "# new_temperature_data = {'sensor3': 27.0, 'sensor4': 28.5}\n",
    "# logger.update_category('temperature', new_temperature_data)\n",
    "# \n",
    "# # Retrieve and print logs\n",
    "# temperature_logs = logger.get_logs('temperature')\n",
    "# print(\"Temperature Logs:\", temperature_logs)\n",
    "\n"
   ],
   "id": "dcad9da0c02c0997",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:13:28.063632Z",
     "start_time": "2025-02-26T16:13:28.058990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    loss_sum = torch.zeros(1, device=device)\n",
    "    all_inputs = torch.zeros(1, device=device)\n",
    "    total_acc = torch.zeros(1, device=device)\n",
    "    with torch.set_grad_enabled(True):\n",
    "        for k, batch in enumerate(loader):\n",
    "            inputs, targets = batch[:-1], batch[-1]\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits[-1], targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            loss_sum += loss.item() * inputs.size(0)\n",
    "            all_inputs += inputs.size(0)\n",
    "            acc = (logits[-1].argmax(-1) == input[-1]).float().mean()\n",
    "            total_acc += acc.item() * input.size[-1]\n",
    "    return total_acc / len(all_inputs), loss_sum / len(all_inputs)\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    loss_sum = torch.zeros(1, device=device)\n",
    "    all_inputs = torch.zeros(1, device=device)\n",
    "    total_acc = torch.zeros(1, device=device)\n",
    "    with torch.no_grad:\n",
    "        for k, batch in enumerate(loader):\n",
    "            inputs, targets = batch[:-1], batch[-1]\n",
    "            inputs, targets = inputs.to_device(), targets.to_device()\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits[-1], targets)\n",
    "            loss_sum += loss.item() * inputs.size(0)\n",
    "            all_inputs += inputs.size(0)\n",
    "            acc = (logits[-1].argmax(-1)) == targets.float().mean()\n",
    "            total_acc += acc.item() * input.size(0)\n",
    "    return total_acc / len(all_inputs), loss_sum / len(all_inputs)"
   ],
   "id": "1ce43baa841bf779",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:13:37.465525Z",
     "start_time": "2025-02-26T16:13:37.462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_optimizer(model, args):\n",
    "    optimizer = getattr(torch.optim, args.optimizer)(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "        betas=(args.beta1, args.beta2),\n",
    "    )\n",
    "\n",
    "    #  linear learning rate warmup over the first 10 updates\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda update: 1 if update > 10 else update / 10\n",
    "    )\n",
    "    return optimizer, scheduler"
   ],
   "id": "a7febbb70f0cb72f",
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:19:42.826400Z",
     "start_time": "2025-02-26T16:19:42.812307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(args):\n",
    "    # info data logging\n",
    "    # Example usage\n",
    "    logger = DataLogger()\n",
    "    logger.log_data('temperature', 'sensor1', 25.5)\n",
    "    logger.log_data('temperature', 'sensor2', 26.0)\n",
    "    logger.log_data('humidity', 'sensor1', 45.0)\n",
    "    \n",
    "    # Retrieve and print logs\n",
    "    temperature_logs = logger.get_logs('temperature')\n",
    "    humidity_logs = logger.get_logs('humidity')\n",
    "    \n",
    "    print(\"Temperature Logs:\", temperature_logs)\n",
    "    print(\"Humidity Logs:\", humidity_logs)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    eq_token = args.p\n",
    "    op_token = args.p + 1\n",
    "    data = multiplication_mod_p_data(p=args.p, eq_token=eq_token, op_token=op_token)\n",
    "    train_loader, test_loader = create_train_test_dataloaders(token_array=data.clone().detach().numpy(), \n",
    "                                                              train_ratio=0.5, batch_size=args.batch_size)\n",
    "    plot_infix = get_plot_infix(args=args)\n",
    "    model = build_model(args=args)\n",
    "    optimizer, scheduler = set_optimizer(model=model, args=args)\n",
    "    steps_per_epoch = math.ceil(data.shape[1] / args.batch_size)\n",
    "    plot_interval = 10\n",
    "    \n",
    "    \n",
    "    for epoch in tqdm(range(int(args.budget // steps_per_epoch))):\n",
    "        trn_acc, trn_loss = train_one_epoch(model=model,loader=train_loader, \n",
    "                                            criterion=torch.nn.CrossEntropyLoss(),\n",
    "                                            optimizer=optimizer, scheduler=scheduler,device=device)\n",
    "        if epoch % 2 == 0:\n",
    "            vld_acc, vld_loss = validate_epoch(model=model,loader=test_loader,)\n",
    "            # info log accuracy and loss for both train and validate\n",
    "            logger.log_data('accuracy', 'train', trn_acc)\n",
    "            logger.log_data('accuracy', 'valid', vld_acc)\n",
    "            logger.log_data('accuracy', 'epoch', epoch)\n",
    "            logger.log_data('loss', 'train', trn_loss)\n",
    "            logger.log_data('loss', 'valid', vld_loss)\n",
    "            logger.log_data('loss', 'epoch', epoch)\n",
    "            norms = get_parameter_norms(model)\n",
    "            logger.update_category_means('norms', norms)\n",
    "            logger.log_data('norms', 'epoch', epoch)\n",
    "        if epoch % plot_interval == 0:\n",
    "            plot_dicts(logger.get_all_logs(), plot_infix=plot_infix)"
   ],
   "id": "6f9244cfd133f6c6",
   "outputs": [],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:19:47.264117Z",
     "start_time": "2025-02-26T16:19:46.372093Z"
    }
   },
   "cell_type": "code",
   "source": "main(args=args)",
   "id": "a8e90d4e25be414e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature Logs: {'sensor1': 25.5, 'sensor2': 26.0}\n",
      "Humidity Logs: {'sensor1': 45.0}\n",
      "Decoder(\n",
      "  (token_embeddings): Embedding(99, 64)\n",
      "  (position_embeddings): Embedding(5, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x Block(\n",
      "      (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=64, out_features=99, bias=False)\n",
      ")\n",
      "Total number of parameters: 113088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15789 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[186], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[185], line 30\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m     26\u001B[0m plot_interval \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mint\u001B[39m(args\u001B[38;5;241m.\u001B[39mbudget \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m steps_per_epoch))):\n\u001B[0;32m---> 30\u001B[0m     trn_acc, trn_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCrossEntropyLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     34\u001B[0m         vld_acc, vld_loss \u001B[38;5;241m=\u001B[39m validate_epoch(model\u001B[38;5;241m=\u001B[39mmodel,loader\u001B[38;5;241m=\u001B[39mtest_loader,)\n",
      "Cell \u001B[0;32mIn[175], line 10\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(model, loader, criterion, optimizer, scheduler, device)\u001B[0m\n\u001B[1;32m      8\u001B[0m inputs, targets \u001B[38;5;241m=\u001B[39m batch[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], batch[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m      9\u001B[0m inputs, targets \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), targets\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 10\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(logits[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], targets)\n\u001B[1;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/miniforge3/envs/mini/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/mini/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[162], line 49\u001B[0m, in \u001B[0;36mDecoder.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     47\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken_embeddings(x)\n\u001B[1;32m     48\u001B[0m positions \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39marange(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], device\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdevice)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 49\u001B[0m h \u001B[38;5;241m=\u001B[39m h \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpositions\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mexpand_as(h)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m     51\u001B[0m     h \u001B[38;5;241m=\u001B[39m layer(h)\n",
      "File \u001B[0;32m~/miniforge3/envs/mini/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/mini/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/mini/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/mini/lib/python3.10/site-packages/torch/nn/functional.py:2237\u001B[0m, in \u001B[0;36membedding\u001B[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[1;32m   2231\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[1;32m   2232\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[1;32m   2233\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m   2234\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[1;32m   2235\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[1;32m   2236\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[0;32m-> 2237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:13.912066Z",
     "start_time": "2025-02-26T16:20:00.822289Z"
    }
   },
   "cell_type": "code",
   "source": "%debug",
   "id": "1115dad64b8e952f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001B[0;32m/Users/igor/miniforge3/envs/mini/lib/python3.10/site-packages/torch/nn/functional.py\u001B[0m(2237)\u001B[0;36membedding\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m   2235 \u001B[0;31m        \u001B[0;31m# remove once script supports set_grad_enabled\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m   2236 \u001B[0;31m        \u001B[0m_no_grad_embedding_renorm_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_norm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnorm_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m-> 2237 \u001B[0;31m    \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscale_grad_by_freq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msparse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m   2238 \u001B[0;31m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m   2239 \u001B[0;31m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n"
     ]
    }
   ],
   "execution_count": 187
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:17:49.927157Z",
     "start_time": "2025-02-26T16:17:49.868444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "steps_per_epoch = math.ceil(data.shape[1] / args.batch_size)\n",
    "int(range(args.budget) // steps_per_epoch)"
   ],
   "id": "8b482e351e428ef0",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[184], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m steps_per_epoch \u001B[38;5;241m=\u001B[39m math\u001B[38;5;241m.\u001B[39mceil(data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m/\u001B[39m args\u001B[38;5;241m.\u001B[39mbatch_size)\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbudget\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m steps_per_epoch)\n",
      "\u001B[0;31mTypeError\u001B[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:24:18.929655Z",
     "start_time": "2025-02-26T16:23:57.262628Z"
    }
   },
   "cell_type": "code",
   "source": "%debug",
   "id": "c86c0f90e1429b08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001B[0;32m/Users/igor/miniforge3/envs/mini/lib/python3.10/site-packages/ipykernel/kernelbase.py\u001B[0m(1325)\u001B[0;36m_input_request\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m   1323 \u001B[0;31m                \u001B[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m   1324 \u001B[0;31m                \u001B[0mmsg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Interrupted by user\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m-> 1325 \u001B[0;31m                \u001B[0;32mraise\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m   1326 \u001B[0;31m            \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m   1327 \u001B[0;31m                \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwarning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Invalid Message:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexc_info\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\n",
      "'Interrupted by user'\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n"
     ]
    }
   ],
   "execution_count": 188
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a1d78ccf0dec6bb5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
